# 深度学习

## 感知机

##### 与门:

```python
#逻辑与
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
        return 0
    else:
        return 1
print("===========") 
print("逻辑与")   
print(AND(0, 0))
print(AND(1, 0))
print(AND(0, 1))
print(AND(1, 1))
print("===========")  
```

##### 带偏置:

```python
import numpy as np
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

##### 或门:

```python
#逻辑或
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.4
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
        return 0
    else:
        return 1
    
print("===========") 
print("逻辑或")   
print(AND(0, 0))
print(AND(1, 0))
print(AND(0, 1))
print(AND(1, 1))
print("===========")  

```

##### 与或门:

(和与门相反)

```python
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

##### 异或门:

(只有一个为真时才为真)

```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```





## 神经网络

#### 阶跃函数:

(简单的0与1的关系)

```python
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x > 0, dtype=int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![image-20230728163209974](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20230728163209974.png)

#### Sigmoid函数:

(平滑性):

```python
import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
    return 1/(1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![image-20230728163526791](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20230728163526791.png)

#### ReLU函数:

> 当值小于零时取零,大于零时取自身

![image-20230728170103180](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20230728170103180.png)

#### 显示图像

> 引入dataset需要将改文件拷贝到当前工作目录下

```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist
from PIL import Image

def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()
    
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False) #一维 ,正规化
img = x_train[0]
label = t_train[0]
print(label)

print(img.shape)
img = img.reshape(28, 28)
print(img.shape)

img_show(img)
```



#### 神经网络的递推处理

```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist
#新增的库文件
#====================
from common.functions import sigmoid, softmax
import pickle

#====================

#获取测试数据集
def get_data():
    (x_train, t_train), (x_test, t_test) = \
        load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return x_test, t_test

#读取文件中记录的权重和偏置
def init_network():
    with open(r"C:\Users\Lenovo\Desktop\Program\Deep Learning\Pyrhon-Program\Neural Network\sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
        
    return network

#验证当前的权重与偏置的正确率
def predict(network, x):
    #先从文件中读取数据
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    #计算 矩阵相乘,带入sigmoid函数,重复
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) +b3
    y = softmax(a3)  #将各数据转化为概率集
    return y

x, t = get_data()
network = init_network()

#记录正确的个数
accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p = np.argmax(y)
    if p == t[i]:
        accuracy_cnt += 1
        
print("Accuracy:"+ str(float(accuracy_cnt)/len(x))) #计算正确的百分比

```



## 神经网络的学习

> 深度学习:计算机自身的学习

#### 损失函数:

> 使用损失值而不是优良程度的原因看书92页

> 一般使用均方误差(对数据都有所涉及)和交叉熵误差(只关注正确结果的计算)

> 损失函数存在的意义是确保结果向最优进行

##### 交叉熵函数:

```python
#y表示各输出值的概率
#t代表正确值标签
#此函数最终只会利用正确值的概率,忽略了其他数据
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

改良版可处理多数据:

```python
#改良版,可以处理单个或多个数据 (one-hot形式)
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    batch_size = y.shape[0] #这里的0表示的是第0维度
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
    #当监督数据为标签形式时
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```



#### 随机取样

```python
#从0-60000中任取10个数字
np.random.choice(60000, 10)
```





#### 梯度法:

> 保证结果向最优进行,但是可能会陷入局部最优,而非全局最优

```python
#其中f为函数, x为一个数组(np.array)
def numerical_gradient(f, x):
    h = 1e-4
    #np.zero_like 用来生成一个与x形状相同的数组
    grad = np.zeros_like(x)
    #遍历x中的数值
    for idx in range(x.size):
        tmp_val = x[idx]
        
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        x[idx] = tmp_val - h
        fxh2 = f(x)
        
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
    
    return grad
```



##### 梯度下降法:

```python
def gradient_descent(f, init_x, lr=0.01, stem_num=100):
    x = init_x
    
    for i in range(stem_num):
        grad = numerical_gradient(f, x)
        x -= lr * grad
        
    return x
```

#### SimpleNet

```python
import sys, os
sys.path.append(pardir)
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient

class simplenet:
    def __init__(self):
        self.W = np.random.randn(2, 3)
        
    def predict(self, x):
        return  np.dot(x, self.W)
    
    def loss(self, x, t):
        z = self.predict(x)
        y = softmax
        loss = cross_entropy_error(y, t)
        
        return loss
```

#### two_layer.class

```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size,
                 weight_init_std=0.01)
    #初始化权证和偏置
    self.params = {}
    
    self.params['W1'] = weight_init_std *\
                        np.random.randn(input_size, hidden_size)
    self.params['b1'] = np.zeros(hidden_size)
    
    self.params['W2'] = weight_init_std *\
                        np.random.randn(hidden_size, output_size)
    self.params['b2'] = np.zeros(output_size)
                        
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = np.dot(x, W1)
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2)
        y = sigmoid(a2)
        
        return y
    
    #计算损失值
    def loss(self, x, t):
        y = self.predict(x)
        
        return cross_entropy_error(y ,t) #?????
    
    #验证正确率
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y==t) / float(x.shape[0])
        return accuracy
       
      #计算梯度
    def numerical_gradient(self, x, t):
        loss_W = lambda W:self.loss(x, t)
        
        grad = {}
        grad['W1'] = numerical_gradient(loss_W,\ 								self.params['W1'])
        grad['b1'] = numerical_gradient(loss_W,\ 								self.params['b1'])
        grad['W2'] = numerical_gradient(loss_W,\								self.params['W2'])
        grad['b2'] = numerical_gradient(loss_W,\ 								self.params['b2'])
        
        return grads
    
    #求梯度高速法
    def gradient(self, x, t):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}
        
        batch_num = x.shape[0]
        
        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        # backward
        dy = (y - t) / batch_num
        grads['W2'] = np.dot(z1.T, dy)
        grads['b2'] = np.sum(dy, axis=0)
        
        da1 = np.dot(dy, W2.T)
        dz1 = sigmoid_grad(a1) * da1
        grads['W1'] = np.dot(x.T, dz1)
        grads['b1'] = np.sum(dz1, axis=0)

        return grads      
```



#### mini-batch

```python
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

(x_train, t_train), (x_test, t_test) = \
        load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1
network = TwoLayerNet(784, 50, 10)

for i in range(iters_num):
    #使用random函数随机抽取两个下标,得到两个数据
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    grad = network.numerical_gradient(x_batch, t_batch) #计算梯度
    
    #更新每一个参数
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
        
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    print(loss)
    

```

cpu:68%



#### epoch_train

```python
#导入数据
(x_train, t_train), (x_test, t_test) =\
         load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []    #损失值
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

#测试数据个数,每个batch的数目,学习速率
iters_num = 10000
batch_size = 100
learning_rate = 0.1

network = TwoLayerNet(784, 50, 10)

for i in range(iters_num):
    
    #随机取样
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    #计算梯度
    grad = network.numerical_gradient(x_batch, t_batch)
    
    #梯度下降法
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate*grad[key]
        
    #计算损失值
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    #if语句判断一个epoch是否结束
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_tets, t_test)
        train_acc_list.append(train_acc)
        tets_acc_list.append(test_acc)
        print("train acc, test acc |" + str(train_acc) + "," +  str(test_acc))
```







#### 学习算法的实现:

取样--梯度--更新



## 误差反向传递





## 学习相关技巧



### Optimizer

#### SGD

```python
# 计算时间过长    SGD：每得到一次梯度就会进行更新
#               BGD：根据多组数据更新，计算量巨大但是准确
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr

    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]               
```



#### Momentum

> \# 与SGD相比，momentum多了一个成员v 类似于小球在滚动，然后对它加速或者减速

```python
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None

    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items:
                self.v[key] = np.zeros_like(val)

        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]

```



#### AdaGrad

> \# AdaGrad与上述两种方法对比 其会记录所有的梯度平方和， 但是缺点是学习到后期时，更新的幅度很小
>
> \# 	由于此缺点，所以可以考虑使用rmspROP代替该方法

```python
class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items:
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr*grads[key] / (np.sqrt(self.h[key]) + 1e-7)  # 防止除以0
```



#### 引用

AdaGrad 和 RMSProp

```python
# 改善上述的代码，通过乘以遗忘速率达到避免后续学习速率过低的情况
# ===============================================================
class RMSprop:
    """RMSprop"""

    def __init__(self, lr=0.01, decay_rate=0.99):
        self.lr = lr
        self.decay_rate = decay_rate
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            self.h[key] *= self.decay_rate
            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
# ===============================================================


# ===============================================================
class Adam:
    """Adam (http://arxiv.org/abs/1412.6980v8)"""

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None

    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)

        self.iter += 1
        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)

        for key in params.keys():
            # self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            # self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key] ** 2 - self.v[key])

            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)

            # unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias
            # unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias
            # params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)
# ===============================================================

```





### 权重初始值

> 通过多种不同的权重初始值来展现了不同的学习速率及特点

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.random.randn(1000, 100)
node_num = 100
hidden_layer_size = 5
activations = {}

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1] 

    # 出现了梯度消失的情况
    # w = np.random.randn(node_num, node_num) * 1

    # 使用不同数值的标准差进行高斯分布，该条件出现了表现力受限的特点
    # w = np.random.randn(node_num, node_num) * 0.01

    #Xavier初始化，与前一层有n个节点连接时，初始值使用标准差为1/sqrt(n) 的分布
    w = np.random.randn(node_num, node_num) /np.sqrt(node_num)

    z = np.dot(x, w)
    a = sigmoid(z)
    activations[i] = a

for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + "-layer")
    plt.hist(a.flatten(), 30, range=(0,1))
plt.show()
```





#### Batch_Norm

> 为了使各层拥有适当的广度， “强制性”地调整激活值的分布

> Batch_Norm 的优点：
>
> ​	使学习快速进行
>
> ​	不非常依赖初始值
>
> ​	抑制过拟合

> Batch_Norm的过程：
>
> ​	顾名思义Batch_Norm以进行学习的mini_batch为单位，按照mini-batch进行正规化。
>
> ​	1.计算m个数据的均值
>
> ​	2.求出数据的方差
>
> ​	3.进行正规化 （x-均值）/ sqrt（方差）
>
> ​	4.之后对正规化的数据进行缩放和平移，参数根据学习进行调整





### 正则化

#### 	过拟合

> 由于神经网络，数据少，参数多，导致出现了过拟合的情况



#### 	权值衰弱

> 学习过程中，对于权重数值大的进行惩罚，来抑制过拟合
>
> 将损失函数加上平方范数，通过对误差函数的要求，来抑制权重过大等问题



>  L2范数相当于各个元素的平方和	L1范数相当于每个权重的绝对值之和	...



#### Dropout

> 在学习时，随机删除神经元，训练时，选出隐藏层的神经元将其删除，之后其不再进行信号的传输，但是在测试时，对于各个神经元的输出，需要乘上训练时删除的比例再输出

```python
import numpy as np
# 抑制过拟合
class Dropout:
    def __init__(self, dropout_radio=0.5):
        self.dropout_radio = dropout_radio
        self.mask = None    # 保存要删除的神经元

    def forward(self, x, train_flg=True):
        if train_flg:
            # 先随机生成和x形状相同的数组，将满足条件的设为True
            self.mask = np.random.randn(*x.shape) > self.dropout_radio
            return x * self.mask
        else:
            return x * (1.0 - self.dropout_radio)
        
    # 与ReLu函数的机制相同
    def backward(self, dout):
        return dout * self.mask
```





#### 超参数的最优化

若要更精炼的方法，可以使用贝叶斯最优化

> ​	1.设定超参数的范围
>
> ​	2.随机取样
>
> ​	3.进行学习，根据数据评估识别精度
>
> ​	4.缩小范围，重复上述步骤



## 卷积神经网络

### 整体结构

> CNN：	卷积层和池化层
>
> 将原来的Affine结构转化为卷积层和池化层

### 卷积层

特征图:	卷积层输入的数据	-> 	输入特征图, 输出特征图

填充： 将图形四周填充一圈/多圈指定的数字

步幅:	滤波器一次移动的大小

通道:	滤波器在三维方向的变量

滤波器的数量: 	滤波器在四维上的变量



### 池化层

池化是缩小 高.长方向上的空间运算

常见的池化层有Max层(可以提取到数据的特殊点), Average层

> 池化层没有需要学习的参数		经过池化运算后,通道数不会发生改变
>
> 对微小的位置变化具有鲁棒性(微小的变化对结果不会产生影响)



### 基于im2col的展开

> image to column

通过im2col将处理的数据展开为二维数据(将应用滤波器的区域横向展开),以适应滤波器的处理

借用numpy中矩阵乘法，实现数据的高速处理



### 卷积层的实现

> im2col函数的接口：
>
> ```python
> # input_data		-由（数据量，通道， 高， 长）的四维数组构成的输入数据
> # filter_h			-滤波器的高
> # filter_w			-滤波器的长
> # stride			-步长
> # pad				-填充
> ```

Convolution_class

```python
class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad

    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)
        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)

        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T
        out = np.dot(col, col_W) + self.b

        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)

        return out
   
```







## 练习:

### 识别手写数字:

> 手写数字识别是深度学习中一个常见且经典的任务。以下是实现手写数字识别的一些基本步骤：
>
> 1. 数据准备：获取手写数字的数据集，常见的数据集包括 MNIST 数据集、Fashion-MNIST 数据集等。将数据集分为训练集和测试集，并进行数据预处理，如归一化处理、重新调整图像大小等。
> 2. 构建神经网络模型：选择适当的神经网络结构，例如卷积神经网络（CNN），用于手写数字识别任务。网络可以包含卷积层、池化层、全连接层等组件。
> 3. 损失函数选择：对于分类任务，常用的损失函数是交叉熵损失函数（Cross Entropy Loss），用于衡量模型输出和真实标签之间的差异。
> 4. 优化器选择：选择合适的优化器，例如随机梯度下降（SGD）、Adam 等，用于更新网络的参数，以最小化损失函数。
> 5. 模型训练：使用训练集对神经网络模型进行训练，通过反向传播算法不断更新参数，直至损失函数收敛或达到预定的训练轮次。
> 6. 模型评估：使用测试集对训练好的模型进行评估，计算模型的准确率、精确率、召回率等指标，评估模型在手写数字识别任务上的性能。
> 7. 可视化结果：可视化模型的预测结果，例如显示模型在测试集上预测正确或错误的样本图像。
> 8. 参数调优：根据评估结果，调整模型的超参数（如学习率、网络结构、正则化等），进一步提高模型的性能。
> 9. 预测新样本：使用训练好的模型对新的手写数字样本进行预测，得到它们的分类结果。
>
> 以上步骤是实现手写数字识别的基本流程。在实践中，还可以进行一些优化和改进，例如使用数据增强技术、迁移学习等，以进一步提高模型的性能和泛化能力

```python
import numpy as np

# 步骤1：准备数据
mnist = np.load('mnist.npz')
train_images, train_labels = mnist['x_train'], mnist['y_train']
test_images, test_labels = mnist['x_test'], mnist['y_test']

# 将图像数据转换为一维向量，并进行归一化处理
train_images = train_images.reshape(-1, 28*28) / 255.0
test_images = test_images.reshape(-1, 28*28) / 255.0

# 步骤2：构建神经网络模型
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

input_size = 28*28
hidden_size = 128
output_size = 10

W1 = np.random.randn(input_size, hidden_size) * 0.01
b1 = np.zeros(hidden_size)
W2 = np.random.randn(hidden_size, output_size) * 0.01
b2 = np.zeros(output_size)

# 步骤3：模型训练
learning_rate = 0.1
epochs = 5
batch_size = 100

for epoch in range(epochs):
    for i in range(0, len(train_images), batch_size):
        # 前向传播
        X = train_images[i:i+batch_size]
        y = train_labels[i:i+batch_size]
        
        hidden = sigmoid(np.dot(X, W1) + b1)
        output = softmax(np.dot(hidden, W2) + b2)
        
        # 计算损失函数（交叉熵损失）
        loss = -np.sum(np.log(output[np.arange(len(y)), y])) / batch_size
        
        # 反向传播
        d_output = output
        d_output[np.arange(len(y)), y] -= 1
        d_output /= batch_size
        
        dW2 = np.dot(hidden.T, d_output)
        db2 = np.sum(d_output, axis=0)
        
        d_hidden = np.dot(d_output, W2.T) * (hidden * (1 - hidden))
        
        dW1 = np.dot(X.T, d_hidden)
        db1 = np.sum(d_hidden, axis=0)
        
        # 参数更新
        W2 -= learning_rate * dW2
        b2 -= learning_rate * db2
        W1 -= learning_rate * dW1
        b1 -= learning_rate * db1
        
    print(f"Epoch {epoch+1}, Loss: {loss}")

# 步骤4：模型评估
hidden = sigmoid(np.dot(test_images, W1) + b1)
output = softmax(np.dot(hidden, W2) + b2)
predicted_labels = np.argmax(output, axis=1)
accuracy = np.mean(predicted_labels == test_labels)
print("Test accuracy:", accuracy)

```



#### 1.数据准备

```python
import numpy as np
mnist = np.load(r"C:\Users\Lenovo\Desktop\Program\Deep Learning\Pyrhon-Program\Demo\mnist.npz")

train_images = mnist['x_train']
train_labels = mnist['y_train']

test_images = mnist['x_test']
test_labels = mnist['y_test']

#将图像转化为一维数据
train_images = train_images.reshape(-1, 28, 28) / 255.0
test_images = test_images.reshape(-1, 28, 28) / 255.0
```



#### 2.搭建神经网络

```python
def sigmoid(x):
    return 1/(1 + np.exp(-x))

#计算概率分布
def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T 

    x = x - np.max(x) # 溢出对策
    return np.exp(x) / np.sum(np.exp(x))
Network = TwoLayerNet(784, 128, 10)
```

